% ----------------------------------------------------------------
% AMS-LaTeX Paper ************************************************
% **** -----------------------------------------------------------
\documentclass{amsart}
\usepackage{graphicx}

\usepackage[latin1]{inputenc}
% ----------------------------------------------------------------
\vfuzz2pt % Don't report over-full v-boxes if over-edge is small
\hfuzz2pt % Don't report over-full h-boxes if over-edge is small
% THEOREMS -------------------------------------------------------
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\numberwithin{equation}{section}
% MATH -----------------------------------------------------------
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\Real}{\mathbb R}
\newcommand{\eps}{\varepsilon}
\newcommand{\To}{\longrightarrow}
\newcommand{\BX}{\mathbf{B}(X)}
\newcommand{\A}{\mathcal{A}}
% ----------------------------------------------------------------
\begin{document}

\section{Mise en place du projet}

\subsection{Organisation du projet}

Ma première tâche au sein du projet a été d'organiser notre
travail à plusieurs niveaux. J'ai réparti le code du projet en
répertoires et en fichiers distinces:
\begin{itemize}
    \item epiEngine est le moteur de stockage de notre base de
    donnée. Il regroupe tout le code gérant les données la lecture et l'écriture des informations.
    \item sql contient l'interprêteur SQL: il transforme les
    requêtes envoyées par le client en une structure directement
    utilisable par le moteur de stockage.
\end{itemize}

\vspace{0.5cm}

 D'autres dossiers ont également été ajoutés: locale
contient tous les messages d'erreurs et plus généralement les
données qui diffèrent selon les langues. www contiendra le site
web lorsqu'il sera réalisé. docs contient la documentation écrite
par l'équipe (rapports, présentations, documentation collectée sur
le net...) et enfin extra contient les programmes supplémentaires
et les archives du projet.

\vspace{0.5cm}

 Nous avons également convenu de respecter au maximum la norme
EPITA, en particulier sur les points suivants:
\begin{itemize}
    \item en-tête des fichiers
    \item nommage des variables
    \item découpage en un maximum de fichiers distincts
\end{itemize}

Même si ces points sont secondaires, il est important de se fixer
dès le départ une ligne de conduite afin d'arriver à un code clair
et lisible à la fin du projet lorsqu'il y a aura beaucoup plus de
débuggage.

\subsection{Types de données}

Mon second travail a consisté à définir les types de données que
nous allons utiliser pour faire communiquer l'interprêteur, le
moteur de stockage et l'interface client/serveur.\\
Désormais, chaque partie sait ce qu'il doit traiter et ce qu'il
doit renvoyer, ainsi chaque membre de l'équipe peut avancer de
manière quasiment indépendante.

\vspace{0.5cm}

 Nous avons deux types de données principalement:
s\_sql\_query qui contient la requête analysée et s\_sql\_result
qui contient le résultat de la requête.

\vspace{0.5cm}

\begin{verbatim}
    #define S_DATABASE      64
    #define S_TABLE         64
    #define S_COLUMN        64
    #define S_INDEX         64
    #define S_ALIAS         256

    typedef char            t_dbname[S_DATABASE-1];
    typedef char            t_tblname[S_TABLE-1];
    typedef char            t_columnname[S_COLUMN-1];
    typedef char            t_indexname[S_INDEX-1];
    typedef char            t_aliasname[S_ALIAS-1];


    typedef struct s_table  *p_table;


    struct                  s_sql_query
    {
       t_dbname             dbname;
       t_tblname            tblname;
       p_table              table;

       int                  if_exists;
       int                  if_not_exists;
    };

    struct s_sql_result
    {
       int                  error_code;
    };

    struct                  s_column
    {
       t_columnname         name;
       unsigned char        size;
       char                 flags;
    };


    struct                  s_table
    {
       struct s_column      column;
       p_table              next;
    };
\end{verbatim}

\vspace{0.5cm}

Pour éviter d'allouer les chaînes dynamiquement, des longueurs
maximums ont été définies pour les tailles des noms de base de
données, tables, colonnes, etc.

\vspace{0.5cm}

On trouve ensuite s\_sql\_query notre type qui contient les
requêtes analysées: il s'étoffera au fur et à mesure que
l'interprêteur évoluera. Il gère actuellement les données
suivantes:

\vspace{0.5cm}

\begin{itemize}
    \item dbname est utilisé pour passer le nom d'une base de
    donnée, par exemple dans CREATE DATABASE.
    \item tblname est utilisé quasiment dans toutes les requêtes (CREATE TABLE, SELECT, INSERT, etc.),
    il indique sur quelle table elle s'applique.
    \item table est une liste chaînée de colonne qui permet de
    décrire la structure d'une table. Il est utilisé dans CREATE
    TABLE. Chaque élément de la liste est une structure contenant
    toutes les informations à propos d'une colonne: nom, taille,
    et drapeaux.
    \item if\_not\_exists et if\_exists sont des drapeaux permettant
    d'autoriser les creations et les suppressions à échouer sans
    émettre d'erreur ni stopper l'execution de la requête.
\end{itemize}

\vspace{0.5cm}

La seconde structure mise en place est s\_sql\_result qui gère le
résultat renvoyé. Elle n'est composée pour l'instant que de
l'entier error\_code qui renvoit un code d'erreur: zéro s'il n'y a
pas de problème ou un nombre supérieur à 0 sinon. Les erreurs sont
indexées et sont liées à une chaîne de caractère via les fichier
du répertoire locale.

\vspace{0.5cm}

Ces deux structures sont instanciées une seule fois dans tout le
programme via deux variables globales: gl\_query et gl\_result.
Cela permet ainsi d'avoir un syntaxe commune à toutes les
fonctions du moteur de stockage:

\begin{verbatim}
void FONCTION(s_sql_query *query, s_sql_result *result);
\end{verbatim}

\vspace{0.5cm}

On peut schématiser le fonctionnement du programme de cette
façon:\\
 \par

 \vspace{0.5cm}
 \includegraphics{schemas_types.jpg}


\newpage
\section{Interprétation des requêtes SQL}

\subsection{Présentation: SQL, les outils flex et bison}

Les requêtes envoyées à notre système de gestion de base de
données utilisent le langage SQL. Même s'il peut gérer des
variables, des boucles, des instructions conditionnelles, etc.
Notre but est d'abord de gérer les requêtes de manipulation
simples. Actuellement l'interpréteur reconnaît les syntaxes
suivantes:

\vspace{0.5cm}

\begin{verbatim}
    CREATE DATABASE [IF NOT EXISTS] `table` ;
    DROP DATABASE   [IF EXISTS] `table` ;

    CREATE TABLE    `table` ( `nom_de_colonne` type_de_colonne ,
    ...) ;

    CHECK TABLE     `table` ;
    OPTIMIZE TABLE  `table` ;

    INSERT INTO `table` ( valeurs, ... ) ;
    SELECT liste_champs FROM `table` ;
\end{verbatim}

\vspace{0.5cm}

Les deux premières requêtes permettent respectivement de créer et
détruire une base de donnée.

La troisième permet de créer une table.

La quatrième et la cinquième de réaliser des opérations de
maintenance.

Enfin la cinquième insère des données dans une table et la
dernière rapatrie toutes les données contenues dans une table.

Les parties entre crochets sont facultatives. Elles permettent de
faire échouer silencieusement les deux premières requêtes.

\vspace{1cm}

Pour analyser les requêtes, nous avons choisi d'utiliser les deux
outils flex et bison qui sont respectivement un analyseur lexical
et sémentique.\\
flex reconnaît via des expressions régulières les différents
éléments du langage:
\begin{description}
    \item[les mots-clés] CREATE, DATABASE, TABLE...
    \item[les identifiants] ma\_table ou `ma\_table`
    \item[les autres types de donnée] nombres (0, 17, 3.14),
    chaînes de caractères ("ma chaîne"), etc.
\end{description}

Chaque élément reconnu est ensuite envoyé à l'analyseur syntaxique
qui reconnaît la grammaire du langage: c'est à dire toutes les
règles qui indiquent si un langage est écrit de manière cohérente:
"CREATE DATABASE `db` ;" a un sens mais pas "CREATE `db` DATABASE
;".

\subsection{Analyseur lexical}

Flex est notre analyseur lexical, il doit découper la requête de
manière logique: il doit reconnaître les mots-clés du langage, les
noms de table, de champs, les chaînes de caractères, etc.

Pour cela, on lui passe en paramètre un fichier comprenant un
ensemble de règles. En voici un extrait:

\vspace{1cm}

\begin{verbatim}
%{
    #include <stdio.h>
    #include <stdlib.h>
    #include <strings.h>

    #include "../types.h"
    #include "sql_tab.h"

    extern YYSTYPE yylval;
%}

%%

    ADD                   return TOKADD;
    ALL                   return TOKALL;
    ALTER                 return TOKALTER;
    ANALYZE               return TOKANALYSE;
    AND                   return TOKAND;
    AS                    return TOKAS;
    [...]

    ;                     return SEMICOLON;
    ,                     return VIRGULE;
    \(                    return PARENTHESEOPEN;
    \)                    return PARENTHESECLOSE;
    \"[^\"]\"+            yylval.string=(char*)strdup(yytext); return STRING;
    '[^']'+               yylval.string=(char*)strdup(yytext); return STRING;

    [0-9]+\.[0-9]*        yylval.floatNumber=atof(yytext); return FLOAT;
    [0-9]+                yylval.number=atoi(yytext); return INTEGER;

    \*                    return JOKER;

    [a-zA-Z0-9]{1,255}    yylval.string=(char*)strdup(yytext); return IDENT;
    `[a-zA-Z0-9]{1,255}`  yylval.string=(char*)strdup(yytext);unquote(yylval.string); return IDENT;

    \n                    /* ignore end of line */;
    [ \t]+                /* ignore whitespace */;
    .                     /* ignore */
%%
\end{verbatim}

\vspace{1cm}

Il est divisé en 4 parties:

\vspace{1cm}

\begin{verbatim}
    %{
    DEFINITIONS
    %}
    %%
    REGLES ET ACTIONS ASSOCIEES
    %%
    FONCTIONS
\end{verbatim}

\vspace{1cm}

La première section contient du code C: les inclusions et les
définitions de variables principalement.

La deuxième section est la plus intéressante, elle se présente
sous la forme d'une expression régulière suivie d'un espace et du
code C à exécuter lorsque l'on reconnaît ce type d'élément. On
retourne la plupart du temps une constante qui permet d'identifier
l'élément. Bison fournit également la variable yylval dans
laquelle on peut stocker une valeur supplémentaire: lorsque l'on
reconnaît un identifiant par exemple, on retourne une constante
indiquant qu'il s'agit d'un identifiant et on mets sa valeur dans
yylval.

Enfin la dernière section inclut des fonctions C supplémentaires.

Le fonctionnement de l'analyseur lexical peut donc se résumer sous
cette forme:

 \par

 \vspace{0.5cm}
 \includegraphics{schema_flex.jpg}

\vspace{2cm}

\subsection{Analyseur syntaxique}

Bison est l'analyseur syntaxique que nous avons utilisé: à partir
de la grammaire du langage, il reconnaît les requêtes qui ont un
sens et exécute le code correspondant.

Voici un extrait de la grammaire utilisée dans le projet:

\vspace{1cm}

\begin{verbatim}
    %{
    [...]
    extern struct s_sql_query  gl_query;
    extern struct s_sql_result gl_result;
    [...]
    %}

    %token TOKADD TOKALL TOKALTER TOKANALYSE TOKAND TOKAS TOKASC TOKASENSITIVE
    [...]

    %union
    {
       double floatNumber;
       int number;
       char *string;
    }

    %token <string>      STRING
    %token <floatNumber> FLOAT
    %token <number>      INTEGER
    %token <string>      IDENT

    %start commands
    %%

    /* ROOT */
    commands:
    /* empty */
    | commands command SEMICOLON
    | commands command
    ;

    /* LIST OF COMMANDS */
    command:
    check_table
    | create_database
    | create_table
    | drop_database
    | insert
    | optimize_table
    | select
    ;

    [...]

    /* === CREATE DATABASE === */
    create_database:
    TOKCREATE TOKDATABASE if_not_exists IDENT
    {
       strcpy(gl_query.dbname,$4);
       createdatabase(&gl_query,&gl_result);
    }
    ;

    [...]

    /* === SHARED STATES === */
    [...]
    if_not_exists
    : /* empty */     { gl_query.if_not_exists = 0; }
    | TOKIF TOKEXISTS { gl_query.if_not_exists = 1; }
    ;
    %%
\end{verbatim}

On peut remarquer que la grammaire se décompose en trois sections,
tout comme la description lexicale. Elles ont également le même
sens: définitions, règles et fonctions.

Cependant quelques points diffèrent: tout t'abord on trouve avant
entre la première et la deuxième partie un ensemble de lignes qui
ne sont pas des règles mais des indications pour bison.

\%token permet d'indiquer la liste des tokens reconnus par flex,
c'est à dire tous les types de données différents que comporte le
langage à analyser.

\%union est une union au sens du C: on définit ici le type de
yylval utilisé par flex pour envoyer des informations à bison.
Comme nous pouvons renvoyer plusieurs types d'informations, on
définit ici une union, ce qui évacue le problème de typage de
yylval.

\%start indique l'état de départ de notre analyseur syntaxique.

\vspace{1cm}

La deuxième section définit la grammaire du langage sous la forme
d'une énumération d'états. A chaque état est associté un motif et
éventuellement des actions. Les motifs sont fortement récursifs.
Prenons par exemples l'état de départ:

\begin{verbatim}
    commands:
    /* empty */
    | commands command SEMICOLON
    | commands command
    ;
\end{verbatim}

Il indique que ce qui est analysé peut présenter trois formes:

\begin{enumerate}
    \item rien
    \item une commande et un point-virgule
    \item une commande uniquement
\end{enumerate}

Dans les cas B et C le motif commence par "commands" soit un appel
récursif à l'état actuel: il signifie que l'on peut avoir avant le
motif les trois cas de l'état. Evidemment, on s'aperçoit ainsi que
l'on peut ainsi analyser un nombre infini de commandes, séparées
par un point-virgule ou pas.

De plus, à n'importe où dans les motifs, on peut ajouter entre
accolade du code C qui s'executera lors de l'analyse du token
précédent l'accolade.


\newpage
\section{Conclusion}
\subsection{Points positifs}

\subsection{Points négatifs}

\subsection{Bilan de la première soutenance}

% ----------------------------------------------------------------
\end{document}
% ----------------------------------------------------------------
